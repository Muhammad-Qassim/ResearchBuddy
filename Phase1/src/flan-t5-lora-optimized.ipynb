{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ac7e8f",
   "metadata": {},
   "source": [
    "# Optimized Fine-tuning FLAN-T5-large with LoRA for Text Summarization\n",
    "\n",
    "optimized for memory efficiency on a 45GB RTX 8000 GPU, specifically for large inputs and outputs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e415d",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92809106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft transformers datasets accelerate evaluate bitsandbytes loralib rouge-score tensorboard py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e581f2fa-ea36-4da8-ac95-38c035eb960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cd46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889fc53",
   "metadata": {},
   "source": [
    "## Memory Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TRANSFORMERS_ENABLE_GRAD_CHECKPOINT\"] = \"true\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd213b5",
   "metadata": {},
   "source": [
    "## Check GPU and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7454ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total GPU memory: 47.76 GB\n",
      "Available GPU memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check available GPU memory\n",
    "if device == 'cuda':\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc70913",
   "metadata": {},
   "source": [
    "## Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa7672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d850cd32c4c08b0c99dff48101fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52d61c",
   "metadata": {},
   "source": [
    "## Configure Parameters\n",
    "These parameters can be adjusted based on your memory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4309c5b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max input length: 9500, max target length: 1200\n",
      "Batch size: 1, Gradient accumulation steps: 8\n"
     ]
    }
   ],
   "source": [
    "MAX_INPUT_LENGTH = 9500  \n",
    "MAX_TARGET_LENGTH = 1200  \n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "EPOCHS = 1.5\n",
    "\n",
    "print(f\"Using max input length: {MAX_INPUT_LENGTH}, max target length: {MAX_TARGET_LENGTH}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe96d0a",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85d5a5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load dataset from JSON file\"\"\"\n",
    "    try:\n",
    "        with open('./Dataset_2.json', 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        dataset = Dataset.from_dict({\n",
    "            'article': [item['article'] for item in data],\n",
    "            'summary': [item['summary'] for item in data]\n",
    "        })\n",
    "        \n",
    "        # Splitting dataset\n",
    "        dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        \n",
    "        print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "        print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "        \n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "263099e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 790\n",
      "Test dataset size: 198\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4fd8d",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ff82bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load model and tokenizer\"\"\"\n",
    "    model_id = \"google/flan-t5-large\"\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load model with 8-bit quantization to save memory\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_id,\n",
    "        load_in_8bit=True,  # Use 8-bit quantization to reduce memory usage\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce4b939d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532aa70",
   "metadata": {},
   "source": [
    "## Preprocess and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a2d25",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample, tokenizer, padding=\"max_length\"):\n",
    "    \"\"\"\n",
    "    Preprocess the data by tokenizing and formatting it correctly for the model.\n",
    "    \"\"\"\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"article\"]]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        padding=padding, \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        text_target=sample[\"summary\"], \n",
    "        max_length=MAX_TARGET_LENGTH, \n",
    "        padding=padding, \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] \n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d870471",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, tokenizer):\n",
    "    \"\"\"Tokenize and prepare the dataset\"\"\"\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: preprocess_function(x, tokenizer), \n",
    "        batched=True, \n",
    "        remove_columns=[\"article\", \"summary\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "    \n",
    "    # Save datasets to disk\n",
    "    os.makedirs(\"flan-t5-lora-optimized/data/train\", exist_ok=True)\n",
    "    os.makedirs(\"flan-t5-lora-optimized/data/eval\", exist_ok=True)\n",
    "    \n",
    "    tokenized_dataset[\"train\"].save_to_disk(\"flan-t5-lora-optimized/data/train\")\n",
    "    tokenized_dataset[\"test\"].save_to_disk(\"flan-t5-lora-optimized/data/eval\")\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62142f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fee653244c4dccb957dd3503792b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/790 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041ca6ec2dfd4567a5f3098a67380415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e8a6ddc58c4282bfc51768d616bd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/790 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb47daf2c7b4e1bba854065233b7ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = prepare_dataset(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30360d68",
   "metadata": {},
   "source": [
    "## Set up LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7ff65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_lora_config(model):\n",
    "    \"\"\"Set up LoRA configuration and apply it to the model\"\"\"\n",
    "    # Configure LoRA for parameter-efficient fine-tuning\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # Rank\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q\", \"v\"], \n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA config to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Enable input gradients for LoRA training\n",
    "    model.enable_input_require_grads()\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf51e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 787,868,672 || trainable%: 0.5989\n"
     ]
    }
   ],
   "source": [
    "model = setup_lora_config(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd2157",
   "metadata": {},
   "source": [
    "## Set up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cadad61",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_training(model, tokenized_dataset, tokenizer, resume_from_checkpoint=None):\n",
    "    \"\"\"Set up training configuration and data collator\"\"\"\n",
    "    label_pad_token_id = -100\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8\n",
    "    )\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    output_dir = \"flan-t5-lora-optimized\"\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=1e-3,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"epoch\",\n",
    "        eval_accumulation_steps=4,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        bf16=True,\n",
    "        fp16=False if torch.cuda.is_bf16_supported() else True,\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"tensorboard\",\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    return trainer, resume_from_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abb87ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: flan-t5-lora-optimized/checkpoint-99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 2:51:44, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.861400</td>\n",
       "      <td>1.550673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=147, training_loss=0.5773324050059935, metrics={'train_runtime': 10494.7955, 'train_samples_per_second': 0.113, 'train_steps_per_second': 0.014, 'total_flos': 5.054225916336538e+16, 'train_loss': 0.5773324050059935, 'epoch': 1.4962025316455696})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "trainer, resume_from_checkpoint = setup_training(model, tokenized_dataset, tokenizer)\n",
    "\n",
    "# Get latest checkpoint\n",
    "last_checkpoint = get_last_checkpoint(\"flan-t5-lora-optimized\")\n",
    "if last_checkpoint:\n",
    "    print(f\"Resuming from checkpoint: {last_checkpoint}\")\n",
    "    resume_from_checkpoint = last_checkpoint\n",
    "\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f33fd7",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec79fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3718b0a27ed4ef6839632c4c5713a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/18.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08063b9bc564f1fbc2e19f9eee4e8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/KASHU101/flan-t5-lora-summarization-optimized/commit/8733cd5319f952266bd60029ccbbf381bf329bbd', commit_message='Upload tokenizer', commit_description='', oid='8733cd5319f952266bd60029ccbbf381bf329bbd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/KASHU101/flan-t5-lora-summarization-optimized', endpoint='https://huggingface.co', repo_type='model', repo_id='KASHU101/flan-t5-lora-summarization-optimized'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "peft_model_id = \"flan-t5-lora-summarization-optimized\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "\n",
    "trainer.model.push_to_hub(peft_model_id)\n",
    "tokenizer.push_to_hub(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123e4c1-b0bb-48de-993e-664f6bab119e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec33122e1dbc40339324f21360d5143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/18.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df1573849604a2391d901ab18d42c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/19.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/KASHU101/flan-t5-lora-summarization-optimized/commit/251a21564c2bfe344eede08e9e3924242a048a66', commit_message='Upload adapter_model.bin with huggingface_hub', commit_description='', oid='251a21564c2bfe344eede08e9e3924242a048a66', pr_url=None, repo_url=RepoUrl('https://huggingface.co/KASHU101/flan-t5-lora-summarization-optimized', endpoint='https://huggingface.co', repo_type='model', repo_id='KASHU101/flan-t5-lora-summarization-optimized'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "\n",
    "safetensors_path = hf_hub_download(\n",
    "    repo_id=\"KASHU101/flan-t5-lora-summarization-optimized\",\n",
    "    filename=\"adapter_model.safetensors\"\n",
    ")\n",
    "\n",
    "weights = load_file(safetensors_path)\n",
    "bin_path = \"adapter_model.bin\"\n",
    "torch.save(weights, bin_path)\n",
    "\n",
    "\n",
    "upload_file(\n",
    "    path_or_fileobj=bin_path,\n",
    "    path_in_repo=\"adapter_model.bin\",\n",
    "    repo_id=\"KASHU101/flan-t5-lora-summarization-optimized\",\n",
    "    token=\"token\" # hugging face token here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f52e7c",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bea4b75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer):\n",
    "    \"\"\"Evaluate the model and compute ROUGE scores\"\"\"\n",
    "    print(\"Evaluating model...\")\n",
    "    metric = evaluate.load(\"rouge\")\n",
    "    \n",
    "    def evaluate_peft_model(sample, max_target_length=MAX_TARGET_LENGTH):\n",
    "        \"\"\"Evaluate the model on a given sample.\"\"\"\n",
    "        # Generate summary\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=sample[\"input_ids\"].unsqueeze(0).to(device), \n",
    "                do_sample=True, \n",
    "                top_p=0.9, \n",
    "                max_new_tokens=max_target_length\n",
    "            )\n",
    "            \n",
    "        prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        \n",
    "        # Decode eval sample\n",
    "        labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "        labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        return prediction, labels\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_dataset = load_from_disk(\"flan-t5-lora-optimized/data/eval/\").with_format(\"torch\")\n",
    "    \n",
    "    # Run predictions on a subset to save time during evaluation\n",
    "    eval_size = min(30, len(test_dataset))\n",
    "    print(f\"Evaluating on {eval_size} samples...\")\n",
    "    \n",
    "    predictions, references = [], []\n",
    "    for i, sample in enumerate(tqdm(test_dataset.select(range(eval_size)))):\n",
    "        p, l = evaluate_peft_model(sample)\n",
    "        predictions.append(p)\n",
    "        references.append(l)\n",
    "    \n",
    "    # Compute metrics\n",
    "    rouge = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"ROUGE-1: {rouge['rouge1'] * 100:.2f}%\")\n",
    "    print(f\"ROUGE-2: {rouge['rouge2'] * 100:.2f}%\")\n",
    "    print(f\"ROUGE-L: {rouge['rougeL'] * 100:.2f}%\")\n",
    "    print(f\"ROUGE-Lsum: {rouge['rougeLsum'] * 100:.2f}%\")\n",
    "    \n",
    "    return rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "badbaabd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluating on 30 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:21:29<00:00, 162.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 55.18%\n",
      "ROUGE-2: 22.03%\n",
      "ROUGE-L: 23.69%\n",
      "ROUGE-Lsum: 23.65%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "rouge_scores = evaluate_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86743e8",
   "metadata": {},
   "source": [
    "## Zip and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edf97545",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: flan-t5-lora-optimized/ (stored 0%)\n",
      "  adding: flan-t5-lora-optimized/data/ (stored 0%)\n",
      "  adding: flan-t5-lora-optimized/data/train/ (stored 0%)\n",
      " (deflated 81%)t5-lora-optimized/data/train/data-00000-of-00001.arrow\n",
      "  adding: flan-t5-lora-optimized/data/train/state.json (deflated 38%)\n",
      "  adding: flan-t5-lora-optimized/data/train/dataset_info.json (deflated 69%)\n",
      "  adding: flan-t5-lora-optimized/data/eval/ (stored 0%)\n",
      " (deflated 80%)t5-lora-optimized/data/eval/data-00000-of-00001.arrow\n",
      "  adding: flan-t5-lora-optimized/data/eval/state.json (deflated 39%)\n",
      "  adding: flan-t5-lora-optimized/data/eval/dataset_info.json (deflated 69%)\n",
      "  adding: flan-t5-lora-optimized/logs/ (stored 0%)\n",
      "  adding: flan-t5-lora-optimized/logs/events.out.tfevents.1741640813.a4ac8b5928f5.1154.0 (deflated 62%)\n",
      "  adding: flan-t5-lora-optimized/logs/events.out.tfevents.1741640956.a4ac8b5928f5.1385.0 (deflated 62%)\n",
      "  adding: flan-t5-lora-optimized/logs/events.out.tfevents.1741641042.a4ac8b5928f5.1385.1 (deflated 62%)\n",
      "  adding: flan-t5-lora-optimized/logs/events.out.tfevents.1741641152.a4ac8b5928f5.1610.0 (deflated 62%)\n",
      "  adding: flan-t5-lora-optimized/logs/events.out.tfevents.1741642604.a4ac8b5928f5.1964.0 (deflated 62%)\n",
      "  adding: flan-t5-lora-optimized/logs/events.out.tfevents.1741643246.a4ac8b5928f5.2238.0 (deflated 60%)\n",
      "  adding: flan-t5-lora-optimized/logs/events.out.tfevents.1741670582.a4ac8b5928f5.5321.0 (deflated 60%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-99/ (stored 0%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-99/README.md (deflated 66%)\n",
      " (deflated 8%)-t5-lora-optimized/checkpoint-99/adapter_model.safetensors\n",
      "  adding: flan-t5-lora-optimized/checkpoint-99/adapter_config.json (deflated 54%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-99/training_args.bin (deflated 52%)\n",
      " (deflated 8%)-t5-lora-optimized/checkpoint-99/optimizer.pt\n",
      "  adding: flan-t5-lora-optimized/checkpoint-99/scheduler.pt (deflated 56%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-99/rng_state.pth (deflated 25%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-99/trainer_state.json (deflated 56%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-147/ (stored 0%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-147/README.md (deflated 66%)\n",
      " (deflated 7%)-t5-lora-optimized/checkpoint-147/adapter_model.safetensors\n",
      "  adding: flan-t5-lora-optimized/checkpoint-147/adapter_config.json (deflated 54%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-147/training_args.bin (deflated 52%)\n",
      " (deflated 8%)-t5-lora-optimized/checkpoint-147/optimizer.pt\n",
      "  adding: flan-t5-lora-optimized/checkpoint-147/scheduler.pt (deflated 56%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-147/rng_state.pth (deflated 25%)\n",
      "  adding: flan-t5-lora-optimized/checkpoint-147/trainer_state.json (deflated 60%)\n"
     ]
    }
   ],
   "source": [
    "# Save everything as a zip file\n",
    "!zip -r flan-t5-lora-optimized.zip flan-t5-lora-optimized/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651acde4",
   "metadata": {},
   "source": [
    "## Inference with the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e48a43f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_path):\n",
    "    \"\"\"Load the fine-tuned model for inference\"\"\"\n",
    "    # Load base model\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        \"google/flan-t5-large\",\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load fine-tuned model\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e524fa0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_summary(text, model, tokenizer):\n",
    "    \"\"\"Generate a summary for the given text\"\"\"\n",
    "    # Prepare the input\n",
    "    inputs = tokenizer(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors=\"pt\", \n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            max_new_tokens=MAX_TARGET_LENGTH,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf8416-3931-46fa-8cd5-5697723680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "RESEARCH PAPER TEXT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bed9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a comprehensive summary of the provided research paper, covering the requested elements: **1. Objective:** * **Main Goal:** The primary objective of this research is to provide a mathematical explanation of UNet, the widely popular deep neural network architecture for image segmentation, by showing the architecture is exactly the model it originated from. * **Significance:** The authors provide a mathematical explanation of UNet, a network created in the UNet architecture, for solving a control problem. They define a control problem describing the dynamics of a neural network and propose a control splitting algorithm to solve it. * **Significance:** The analysis provides a mathematical explanation of how a UNet network is solved. They intend to directly link UNet to a network-specific framework, providing a more comprehensive and specific explanation for this algorithm than traditional algorithms that use generic techniques such as equational methods or recursion. * **Mathematical Foundations:** The paper relies on key concepts from earlier research, particularly the correlations between encoding and decoding algorithms. The underlying assumptions are key: * **Constrained control problem:** The problem is u(x,t) t = W (x, t)  u(x, t) + d(t)  ln u(x,t)  0, u(x, 0) = H(f), x  . * **Internal value problem:** U(x, 0) = H(f), x   (, T). * **Sequential Splitting Method:** The author introduces the hybrid splitting method to solve the constrained control problem. * **Hybrid Splitting Method:** The algorithm incorporates several steps of sequential splitting and parallel splitting. * **Multigrid Method:** The control variables are decomposed using multigrid methods. * **Operator Splitting Technique:** The authors propose a hybrid splitting technique where operators (K,s, Sm) split the control variables based on the varying scales of the multigrids. **2. Methodology:** The paper uses a hybrid splitting method that is a mixture of sequential splitting and parallel splitting. The method utilizes the multigrid idea to decompose control variables into components with different scales. * **Operator Splitting:** The method combines the three steps described above, and in particular the downsampling and upsampling operations. The problem is split into M sequential steps, with each sub-step solving cm parallel pathways with cm parallel paths. **3. Key Findings:** * **Control Problem:** The paper demonstrates that UNet is a one-step operator-splitting algorithm solving a control problem. * **Unconstrained Control Problem:** The problem is considered to be an encoder-decoder-based network based on a constrained optimization problem. * **No Constrained Optimization Problem:** The constrained control problem is derived from a control problem u(x, 0) = u0(x), k=1 s=1 Am k,s(x, t; u) + cm(cid:88) k=1 Sm k (x, t; u) + cm(cid:88) k=1 (cM = 1, Am) where cmM k are operators, fmm k's are some given functions independent of u. * **Differences between Operator Splitting and Generalization:** The authors acknowledge that deep learning and deep neural networks are often integrated together in a more general framework, which provides a more general, more general method. **4. Conclusion:** * **Broader Implications:** The research demonstrates that the UNet architecture is completely reversible after only a single iteration of the algorithm. It also demonstrates the accuracy of the algorithm, suggesting a more general approach. The authors conclude that UNet is a one-step operator-splitting algorithm, reflecting the complexity of the control problem. The research makes an important contribution to a more general understanding of the relationship between deep neural networks and control problems. * **PottsMGNet:** The PottsMGNet was proposed to incorporate the Potts model, operator-splitting methods, control problem, and multigrid method. It demonstrated that potently performing segmenting of images with various noise levels using a single network. This work demonstrates that UNet represents a more general, more general case of the PottsMGNet algorithm. **5. Key Findings:** * **Evidence:** The proposed UNet algorithm approximates the architecture of the original UNet network by calculating a loss function over the decomposed control variables. * **Analysis of the \"Constrained Control Problem\":** The authors used the Potts model and operator-splitting method to solve the constrained control problem. The method converges with first-order accuracy when all operators are linear and in a linearized manner. * **Examples:** The proposed algorithm demonstrates how the constrained control problem behaves after the downsampling and upsampling operations. It demonstrates that the operator-splitting method exacts the UNet architecture. * **Proble\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "inference_model, inference_tokenizer = load_finetuned_model(\"flan-t5-lora-summarization-optimized\")\n",
    "\n",
    "summary = generate_summary(sample_text, inference_model, inference_tokenizer)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
