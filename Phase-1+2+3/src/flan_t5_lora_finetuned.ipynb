{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning FLAN-T5-base with LoRA for Text Summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft==0.2.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from peft==0.2.0) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from peft==0.2.0) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from peft==0.2.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from peft==0.2.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from peft==0.2.0) (2.6.0+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from peft==0.2.0) (4.48.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from peft==0.2.0) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.2.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.2.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.2.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.2.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.2.0) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.2.0) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.2.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.2.0) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from accelerate->peft==0.2.0) (0.28.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from accelerate->peft==0.2.0) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from transformers->peft==0.2.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from transformers->peft==0.2.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from transformers->peft==0.2.0) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from transformers->peft==0.2.0) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers->peft==0.2.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.2.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from requests->transformers->peft==0.2.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from requests->transformers->peft==0.2.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from requests->transformers->peft==0.2.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from requests->transformers->peft==0.2.0) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [49 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: py7zr in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from rouge-score) (2.2.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tensorboard) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tensorboard) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tensorboard) (75.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: texttable in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (3.21.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (0.16.2)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (1.1.1)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (1.0.3)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (1.0.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from py7zr) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\muham\\desktop\\delete\\practice\\.venv\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install \"peft==0.2.0\"\n",
    "#%pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "#%pip install rouge-score tensorboard py7zr \n",
    "#%pip install peft.utils\n",
    "#%pip install -U peft transformers torch bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, TrainingArguments, Trainer , Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "Load dataset from JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./scientific_papers_dataset.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\n",
    "    'article': [item['article'] for item in data],\n",
    "    'summary': [item['summary'] for item in data]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 472\n",
      "Test dataset size: 118\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Device and Model\n",
    "\n",
    "Check for GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set model ID and load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Set maximum lengths for input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 472/472 [00:10<00:00, 46.07 examples/s]\n",
      "Map: 100%|██████████| 118/118 [00:01<00:00, 64.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 472/472 [00:00<00:00, 59000.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 118/118 [00:00<00:00, 19654.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 512\n",
    "\n",
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    \"\"\"\n",
    "    Preprocess the data by tokenizing and formatting it correctly for the model.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing the original data.\n",
    "        padding (str): A string indicating the padding method.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized data.\n",
    "    \"\"\"\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"article\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"article\", \"summary\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"../practice/lora-flan-t5-base/data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"../practice/lora-flan-t5-base/data/eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model with LoRA Configuration\n",
    "\n",
    "Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r=16: Defines the rank of the LoRA update matrices. A lower rank means fewer trainable parameters, reducing memory usage.\n",
    "\n",
    "lora_alpha=32: Scaling factor that helps balance the impact of LoRA weights.\n",
    "\n",
    "target_modules=[\"q\", \"v\"]: Specifies which layers to apply LoRA to:\n",
    "\n",
    "    \"q\" (query) and \"v\" (value) layers in the attention mechanism are modified.\n",
    "\n",
    "lora_dropout=0.05: Introduces dropout to prevent overfitting.\n",
    "\n",
    "bias=\"none\": Ensures that only LoRA weights are modified.\n",
    "\n",
    "task_type=TaskType.SEQ_2_SEQ_LM: Specifies that the model is for sequence-to-sequence learning (like summarization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Configuration\n",
    "\n",
    "Configure data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "label_pad_token_id = -100\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training argument and Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"lora-flan-t5-base\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\t\tauto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Train and Save Model\n",
    " Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muham\\Desktop\\delete\\practice\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\muham\\Desktop\\delete\\practice\\.venv\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 11:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=295, training_loss=2.222763837394068, metrics={'train_runtime': 703.4189, 'train_samples_per_second': 3.355, 'train_steps_per_second': 0.419, 'total_flos': 1628855882219520.0, 'train_loss': 2.222763837394068, 'epoch': 5.0})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results\\\\tokenizer_config.json',\n",
       " 'results\\\\special_tokens_map.json',\n",
       " 'results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id=\"../practice/lora-flan-t5-base/model\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Trained Model\n",
    "\n",
    "Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"../practice/lora-flan-t5-base/model\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "Load ROUGE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [57:09<00:00, 29.06s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue1: 50.101581%\n",
      "rouge2: 20.299048%\n",
      "rougeL: 29.766023%\n",
      "rougeLsum: 29.769431%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_peft_model(sample,max_target_length=512):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given sample.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A dictionary containing the original data.\n",
    "        max_target_length (int): The maximum length of the target sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the prediction and the reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "\n",
    "# load test dataset from distk\n",
    "test_dataset = load_from_disk(\"../practice/lora-flan-t5-base/data/eval/\").with_format(\"torch\")\n",
    "\n",
    "# run predictions\n",
    "predictions, references = [] , []\n",
    "for sample in tqdm(test_dataset):\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric \n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results \n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
